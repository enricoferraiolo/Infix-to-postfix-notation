{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Description:\n",
        "\n",
        "The purpose of this project is to implement a neural network that performs the translation of mathematical formulae from traditional **infix notation**—where the operator appears between two operands—to **postfix** (also known as Reverse Polish Notation), where the operator follows the operands.\n",
        "\n",
        "Infix notation is the most commonly used in human-readable mathematics (e.g., a + b), but it is inherently ambiguous without additional syntactic aids such as parentheses or operator precedence rules. This ambiguity arises because different parse trees can correspond to the same expression depending on how operations are grouped.\n",
        "\n",
        "In contrast, postfix notation eliminates the need for parentheses entirely. The order of operations is explicitly encoded by the position of the operators relative to the operands, making it more suitable for stack-based evaluation and easier to parse programmatically.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Consider the ambiguous infix expression:\n",
        "a + b * c\n",
        "\n",
        "This expression can be parsed in at least two different ways:\n",
        "\n",
        "Interpretation (Infix):\t(a + b) * c\t   \n",
        "Equivalent Postfix: ab+c*\n",
        "\n",
        "Interpretation (Infix):\ta + (b * c)\t          \n",
        "Equivalent Postfix: abc*+\n",
        "\n",
        "\n",
        "This project aims to learn such disambiguations and generate the correct postfix form from a given infix expression using a data-driven approach based on neural networks. To simplify the task and control the complexity of expressions, we restrict our dataset to formulae with a maximum syntactic depth of 3. This means that the abstract syntax trees representing these expressions will have at most three levels, ensuring that the neural network operates on a bounded and manageable set of possible structures."
      ],
      "metadata": {
        "id": "uPFPtHankgU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "i_tRkF6n6smU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We build formulae using 5 identifiers a,b,c,d,e and 4 binary operators +,-,*,/.\n",
        "For simplicity we do not take advantage of precedence or associativity rules for infix notation, and suppose that all binary operations as always fully parenthesizes: (e1 op e2)."
      ],
      "metadata": {
        "id": "QFSHpEHjpa1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Constants --------------------\n",
        "OPERATORS = ['+', '-', '*', '/']\n",
        "IDENTIFIERS = list('abcde')\n",
        "SPECIAL_TOKENS = ['PAD', 'SOS', 'EOS']\n",
        "SYMBOLS = ['(', ')', '+', '-', '*', '/']\n",
        "VOCAB = SPECIAL_TOKENS + SYMBOLS + IDENTIFIERS + ['JUNK'] #may use junk in autoregressive generation\n",
        "\n",
        "token_to_id = {tok: i for i, tok in enumerate(VOCAB)}\n",
        "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "PAD_ID = token_to_id['PAD']\n",
        "EOS_ID = token_to_id['EOS']\n",
        "SOS_ID = token_to_id['SOS']\n",
        "\n",
        "MAX_DEPTH = 3\n",
        "MAX_LEN = 4*2**MAX_DEPTH -2 #enough to fit expressions at given depth (+ EOS)"
      ],
      "metadata": {
        "id": "IINM81OK61pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Expression Generation --------------------\n",
        "def generate_infix_expression(max_depth):\n",
        "    if max_depth == 0:\n",
        "        return random.choice(IDENTIFIERS)\n",
        "    elif random.random() < 0.5:\n",
        "        return generate_infix_expression(max_depth - 1)\n",
        "    else:\n",
        "        left = generate_infix_expression(max_depth - 1)\n",
        "        right = generate_infix_expression(max_depth - 1)\n",
        "        op = random.choice(OPERATORS)\n",
        "        return f'({left} {op} {right})'\n",
        "\n",
        "def tokenize(expr):\n",
        "    return [c for c in expr if c in token_to_id]\n",
        "\n",
        "def infix_to_postfix(tokens):\n",
        "    precedence = {'+': 1, '-': 1, '*': 2, '/': 2}\n",
        "    output, stack = [], []\n",
        "    for token in tokens:\n",
        "        if token in IDENTIFIERS:\n",
        "            output.append(token)\n",
        "        elif token in OPERATORS:\n",
        "            while stack and stack[-1] in OPERATORS and precedence[stack[-1]] >= precedence[token]:\n",
        "                output.append(stack.pop())\n",
        "            stack.append(token)\n",
        "        elif token == '(':\n",
        "            stack.append(token)\n",
        "        elif token == ')':\n",
        "            while stack and stack[-1] != '(':\n",
        "                output.append(stack.pop())\n",
        "            stack.pop()\n",
        "    while stack:\n",
        "        output.append(stack.pop())\n",
        "    return output\n",
        "\n",
        "def encode(tokens, max_len=MAX_LEN):\n",
        "    ids = [token_to_id[t] for t in tokens] + [EOS_ID]\n",
        "    return ids + [PAD_ID] * (max_len - len(ids))\n",
        "\n",
        "def decode_sequence(token_ids, id_to_token, pad_token='PAD', eos_token='EOS'):\n",
        "    \"\"\"\n",
        "    Converts a list of token IDs into a readable string by decoding tokens.\n",
        "    Stops at the first EOS token if present, and ignores PAD tokens.\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    for token_id in token_ids:\n",
        "        token = id_to_token.get(token_id, '?')\n",
        "        if token == eos_token:\n",
        "            break\n",
        "        if token != pad_token:\n",
        "            tokens.append(token)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def generate_dataset(n,max_depth=MAX_DEPTH):\n",
        "    X, Y = [], []\n",
        "    for _ in range(n):\n",
        "        expr = generate_infix_expression(MAX_DEPTH)\n",
        "        #expr = expr_gen.generate(max_depth=max_dthep)\n",
        "        infix = tokenize(expr)\n",
        "        postfix = infix_to_postfix(infix)\n",
        "        X.append(encode(infix))\n",
        "        Y.append(encode(postfix))\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "#you might use the shift function for teacher-forcing\n",
        "def shift_right(seqs):\n",
        "    shifted = np.zeros_like(seqs)\n",
        "    shifted[:, 1:] = seqs[:, :-1]\n",
        "    shifted[:, 0] = SOS_ID\n",
        "    return shifted"
      ],
      "metadata": {
        "id": "T-fO911d6_FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us define a simple dataset, and inspect a few samples."
      ],
      "metadata": {
        "id": "DENVmP3Jq5Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = generate_dataset(10000)\n",
        "decoder_input_train = shift_right(Y_train)\n",
        "\n",
        "# Dataset\n",
        "X_val, Y_val = generate_dataset(1000)\n",
        "decoder_input_val = shift_right(Y_val)"
      ],
      "metadata": {
        "id": "gdlonKn47dE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i =  np.random.randint(10000)\n",
        "print(i)\n",
        "print(\"infix : \",decode_sequence(X_train[i],id_to_token))\n",
        "print(\"posfix notation: \",decode_sequence(Y_train[i],id_to_token))\n",
        "print(\"teacher forcing : \", decode_sequence(decoder_input_train[i],id_to_token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TogClrT6F2Th",
        "outputId": "7ef901de-8fe1-4724-870e-d755135aeb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1211\n",
            "infix :  d\n",
            "posfix notation:  d\n",
            "teacher forcing :  SOS d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constraints\n",
        "* You may use any architecture (decoder-only, encoder-decoder, or other).\n",
        "\n",
        "* The maximum number of parameters is 2 million.\n",
        "\n",
        "* Beam search is not allowed.\n",
        "\n",
        "* You may adapt the formula generator to your needs, but preserve its core logic—especially the frequency distribution of formulas by depth, as it may significantly influence model performance.\n",
        "\n",
        "* You may train your model using a pre-generated fixed dataset (e.g., an array) or directly use an on-the-fly generator.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MgqDkVaztBuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "We shall evaluate a generated item y_pred using \"prefix accuracy\", the lenght of\n",
        "the initial prefix of y_pred matching the ground true y_true. This will be divided by the maximum length of y_true and y_pred (up to EOS), so that a perfect match has score 1.\n",
        "\n",
        "* It's more informative than exact match (which is often 0)\n",
        "\n",
        "* It’s tighter than edit distance: focuses on generation flow\n",
        "\n",
        "* Captures where the model starts to make errors\n",
        "\n"
      ],
      "metadata": {
        "id": "QDUjK4SGvT0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prefix_accuracy_single(y_true, y_pred, id_to_token, eos_id=EOS_ID, verbose=False):\n",
        "    t_str = decode_sequence(y_true, id_to_token).split(' EOS')[0]\n",
        "    p_str = decode_sequence(y_pred, id_to_token).split(' EOS')[0]\n",
        "    t_tokens = t_str.strip().split()\n",
        "    p_tokens = p_str.strip().split()\n",
        "    max_len = max(len(t_tokens), len(p_tokens))\n",
        "\n",
        "    match_len = sum(x == y for x, y in zip(t_tokens, p_tokens))\n",
        "    score = match_len / max_len if max_len>0 else 0\n",
        "\n",
        "    if verbose:\n",
        "        print(\"TARGET :\", ' '.join(t_tokens))\n",
        "        print(\"PREDICT:\", ' '.join(p_tokens))\n",
        "        print(f\"PREFIX MATCH: {match_len}/{len(t_tokens)} → {score:.2f}\")\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "MeqyasiYxCpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the exam, evaluate you model on a test set of 20 expressions. Repeat this evaluation 10 times, and return the mean and std for this rounds."
      ],
      "metadata": {
        "id": "HeCRiqvsxQax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(no=20,rounds=10):\n",
        "  rscores =[]\n",
        "  for i in range(rounds):\n",
        "    print(\"round=\",i)\n",
        "    X_test, Y_test = generate_dataset(no)\n",
        "    scores = []\n",
        "    for j in range(no):\n",
        "      encoder_input=X_test[j]\n",
        "      generated = autoregressive_decode(model, encoder_input)[1:] #remove SOS\n",
        "      scores.append(prefix_accuracy_single(Y_test[j], generated, id_to_token))\n",
        "    rscores.append(np.mean(scores))\n",
        "  return np.mean(rscores),np.std(rscores)\n",
        "\n",
        "res, std = test(20,10)\n",
        "print(\"score=\",res,\"std=\",std)"
      ],
      "metadata": {
        "id": "aR-9eTs28x4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be sure to evalutate the generator: your model may only take as input the expression in infix format and return its translation to postifix.\n",
        "\n",
        "If you are usuing an encoder-decoder model, generation must be done autoregressively."
      ],
      "metadata": {
        "id": "AxxXPqKQ86fZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What to deliver\n",
        "\n",
        "As usual you are supposed to deliver a single notebook witten in Keras. You are auhtorized to use Keras3 with pytorch as backend if your prefer.\n",
        "\n",
        "Do no upload a zip file: the submission will be rejected.\n",
        "\n",
        "The python notebook should have a clear documentation of the training phase, possibly with its history.\n",
        "\n",
        "You should be able to provide the network paramters upon request. Even better, consider a way to upload them inside your notebook using gdown."
      ],
      "metadata": {
        "id": "aOBottQI9o1h"
      }
    }
  ]
}