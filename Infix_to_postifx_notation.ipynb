{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPFPtHankgU8"
   },
   "source": [
    "# Project Description:\n",
    "\n",
    "The purpose of this project is to implement a neural network that performs the translation of mathematical formulae from traditional **infix notation**—where the operator appears between two operands—to **postfix** (also known as Reverse Polish Notation), where the operator follows the operands.\n",
    "\n",
    "Infix notation is the most commonly used in human-readable mathematics (e.g., a + b), but it is inherently ambiguous without additional syntactic aids such as parentheses or operator precedence rules. This ambiguity arises because different parse trees can correspond to the same expression depending on how operations are grouped.\n",
    "\n",
    "In contrast, postfix notation eliminates the need for parentheses entirely. The order of operations is explicitly encoded by the position of the operators relative to the operands, making it more suitable for stack-based evaluation and easier to parse programmatically.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider the ambiguous infix expression:\n",
    "a + b * c\n",
    "\n",
    "This expression can be parsed in at least two different ways:\n",
    "\n",
    "Interpretation (Infix):\t(a + b) * c\t   \n",
    "Equivalent Postfix: ab+c*\n",
    "\n",
    "Interpretation (Infix):\ta + (b * c)\t          \n",
    "Equivalent Postfix: abc*+\n",
    "\n",
    "\n",
    "This project aims to learn such disambiguations and generate the correct postfix form from a given infix expression using a data-driven approach based on neural networks. To simplify the task and control the complexity of expressions, we restrict our dataset to formulae with a maximum syntactic depth of 3. This means that the abstract syntax trees representing these expressions will have at most three levels, ensuring that the neural network operates on a bounded and manageable set of possible structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "i_tRkF6n6smU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 18:02:31.103378: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 18:02:31.106494: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 18:02:31.115455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749657751.131387  309334 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749657751.135776  309334 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749657751.147290  309334 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749657751.147316  309334 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749657751.147319  309334 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749657751.147321  309334 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-11 18:02:31.151078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFSHpEHjpa1x"
   },
   "source": [
    "We build formulae using 5 identifiers a,b,c,d,e and 4 binary operators +,-,*,/.\n",
    "For simplicity we do not take advantage of precedence or associativity rules for infix notation, and suppose that all binary operations as always fully parenthesizes: (e1 op e2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IINM81OK61pH"
   },
   "outputs": [],
   "source": [
    "# -------------------- Constants --------------------\n",
    "OPERATORS = ['+', '-', '*', '/']\n",
    "IDENTIFIERS = list('abcde')\n",
    "SPECIAL_TOKENS = ['PAD', 'SOS', 'EOS']\n",
    "SYMBOLS = ['(', ')', '+', '-', '*', '/']\n",
    "VOCAB = SPECIAL_TOKENS + SYMBOLS + IDENTIFIERS + ['JUNK'] #may use junk in autoregressive generation\n",
    "\n",
    "token_to_id = {tok: i for i, tok in enumerate(VOCAB)}\n",
    "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "PAD_ID = token_to_id['PAD']\n",
    "EOS_ID = token_to_id['EOS']\n",
    "SOS_ID = token_to_id['SOS']\n",
    "\n",
    "MAX_DEPTH = 3\n",
    "MAX_LEN = 4*2**MAX_DEPTH -2 #enough to fit expressions at given depth (+ EOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T-fO911d6_FW"
   },
   "outputs": [],
   "source": [
    "# -------------------- Expression Generation --------------------\n",
    "def generate_infix_expression(max_depth):\n",
    "    if max_depth == 0:\n",
    "        return random.choice(IDENTIFIERS)\n",
    "    elif random.random() < 0.5:\n",
    "        return generate_infix_expression(max_depth - 1)\n",
    "    else:\n",
    "        left = generate_infix_expression(max_depth - 1)\n",
    "        right = generate_infix_expression(max_depth - 1)\n",
    "        op = random.choice(OPERATORS)\n",
    "        return f'({left} {op} {right})'\n",
    "\n",
    "def tokenize(expr):\n",
    "    return [c for c in expr if c in token_to_id]\n",
    "\n",
    "def infix_to_postfix(tokens):\n",
    "    precedence = {'+': 1, '-': 1, '*': 2, '/': 2}\n",
    "    output, stack = [], []\n",
    "    for token in tokens:\n",
    "        if token in IDENTIFIERS:\n",
    "            output.append(token)\n",
    "        elif token in OPERATORS:\n",
    "            while stack and stack[-1] in OPERATORS and precedence[stack[-1]] >= precedence[token]:\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token)\n",
    "        elif token == '(':\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            while stack and stack[-1] != '(':\n",
    "                output.append(stack.pop())\n",
    "            stack.pop()\n",
    "    while stack:\n",
    "        output.append(stack.pop())\n",
    "    return output\n",
    "\n",
    "def encode(tokens, max_len=MAX_LEN):\n",
    "    ids = [token_to_id[t] for t in tokens] + [EOS_ID]\n",
    "    return ids + [PAD_ID] * (max_len - len(ids))\n",
    "\n",
    "def decode_sequence(token_ids, id_to_token, pad_token='PAD', eos_token='EOS'):\n",
    "    \"\"\"\n",
    "    Converts a list of token IDs into a readable string by decoding tokens.\n",
    "    Stops at the first EOS token if present, and ignores PAD tokens.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for token_id in token_ids:\n",
    "        token = id_to_token.get(token_id, '?')\n",
    "        if token == eos_token:\n",
    "            break\n",
    "        if token != pad_token:\n",
    "            tokens.append(token)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def generate_dataset(n,max_depth=MAX_DEPTH):\n",
    "    X, Y = [], []\n",
    "    for _ in range(n):\n",
    "        expr = generate_infix_expression(MAX_DEPTH)\n",
    "        #expr = expr_gen.generate(max_depth=max_dthep)\n",
    "        infix = tokenize(expr)\n",
    "        postfix = infix_to_postfix(infix)\n",
    "        X.append(encode(infix))\n",
    "        Y.append(encode(postfix))\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "#you might use the shift function for teacher-forcing\n",
    "def shift_right(seqs):\n",
    "    shifted = np.zeros_like(seqs)\n",
    "    shifted[:, 1:] = seqs[:, :-1]\n",
    "    shifted[:, 0] = SOS_ID\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DENVmP3Jq5Zf"
   },
   "source": [
    "Let us define a simple dataset, and inspect a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gdlonKn47dE7"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = generate_dataset(10000)\n",
    "decoder_input_train = shift_right(Y_train)\n",
    "\n",
    "# Dataset\n",
    "X_val, Y_val = generate_dataset(1000)\n",
    "decoder_input_val = shift_right(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TogClrT6F2Th",
    "outputId": "7ef901de-8fe1-4724-870e-d755135aeb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4939\n",
      "infix :  a\n",
      "posfix notation:  a\n",
      "teacher forcing :  SOS a\n"
     ]
    }
   ],
   "source": [
    "i =  np.random.randint(10000)\n",
    "print(i)\n",
    "print(\"infix : \",decode_sequence(X_train[i],id_to_token))\n",
    "print(\"posfix notation: \",decode_sequence(Y_train[i],id_to_token))\n",
    "print(\"teacher forcing : \", decode_sequence(decoder_input_train[i],id_to_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgqDkVaztBuv"
   },
   "source": [
    "# Constraints\n",
    "* You may use any architecture (decoder-only, encoder-decoder, or other).\n",
    "\n",
    "* The maximum number of parameters is 2 million.\n",
    "\n",
    "* Beam search is not allowed.\n",
    "\n",
    "* You may adapt the formula generator to your needs, but preserve its core logic—especially the frequency distribution of formulas by depth, as it may significantly influence model performance.\n",
    "\n",
    "* You may train your model using a pre-generated fixed dataset (e.g., an array) or directly use an on-the-fly generator.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDUjK4SGvT0s"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "We shall evaluate a generated item y_pred using \"prefix accuracy\", the lenght of\n",
    "the initial prefix of y_pred matching the ground true y_true. This will be divided by the maximum length of y_true and y_pred (up to EOS), so that a perfect match has score 1.\n",
    "\n",
    "* It's more informative than exact match (which is often 0)\n",
    "\n",
    "* It’s tighter than edit distance: focuses on generation flow\n",
    "\n",
    "* Captures where the model starts to make errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MeqyasiYxCpU"
   },
   "outputs": [],
   "source": [
    "def prefix_accuracy_single(y_true, y_pred, id_to_token, eos_id=EOS_ID, verbose=False):\n",
    "    t_str = decode_sequence(y_true, id_to_token).split(' EOS')[0]\n",
    "    p_str = decode_sequence(y_pred, id_to_token).split(' EOS')[0]\n",
    "    t_tokens = t_str.strip().split()\n",
    "    p_tokens = p_str.strip().split()\n",
    "    max_len = max(len(t_tokens), len(p_tokens))\n",
    "\n",
    "    match_len = sum(x == y for x, y in zip(t_tokens, p_tokens))\n",
    "    score = match_len / max_len if max_len>0 else 0\n",
    "\n",
    "    if verbose:\n",
    "        print(\"TARGET :\", ' '.join(t_tokens))\n",
    "        print(\"PREDICT:\", ' '.join(p_tokens))\n",
    "        print(f\"PREFIX MATCH: {match_len}/{len(t_tokens)} → {score:.2f}\")\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeCRiqvsxQax"
   },
   "source": [
    "For the exam, evaluate you model on a test set of 20 expressions. Repeat this evaluation 10 times, and return the mean and std for this rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "For this project I decided to use a simple Transformer-based sequence-to-sequence model for the given task.\n",
    "\n",
    "This architecture is optimal for this type of sequence-to-sequence task, as it can effectively learn the relationships between the input infix expressions and their corresponding postfix representations.\n",
    "\n",
    "I implemented a encoder-decoder architecture which uses my class `PositionalEncoding` to add positional information to the input embeddings.\n",
    "\n",
    "The model is built as follows:\n",
    "Encoder:\n",
    "- Input Layer: Accepts the tokenized infix expressions.\n",
    "- Embedding Layer: Converts tokens into dense vectors.\n",
    "- Positional Encoding: Adds positional information to the embeddings using the `PositionalEncoding` class.\n",
    "- Encoder Block(s):\n",
    "    - Multi-Head Self-Attention\n",
    "    - Feed Forward Network\n",
    "    - Layer Normalization\n",
    "\n",
    "Decoder:\n",
    "- Input Layer: Accepts the tokenized postfix expressions.\n",
    "- Embedding Layer: Converts tokens into dense vectors.\n",
    "- Positional Encoding: Adds positional information to the embeddings using the `PositionalEncoding` class.\n",
    "- Decoder Block(s):\n",
    "    - Multi-Head Self-Attention\n",
    "    - Encoder-Decoder Attention\n",
    "    - Feed Forward Network\n",
    "    - Layer Normalization\n",
    "\n",
    "Output:\n",
    "- Final Dense Layer: Final Dense layer with softmax to produce the output tokens.\n",
    "```python\n",
    "# Create the model\n",
    "model = create_transformer_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    embedding_dim=64,  # embedding dimension\n",
    "    num_heads=4,  # Number of attention heads\n",
    "    feedforward_dimension=128,  # Feed-forward dimension\n",
    "    num_encoder_layers=1,  # Number of encoder layers\n",
    "    num_decoder_layers=1,  # Number of decoder layers\n",
    ")\n",
    "```\n",
    "These parameters were chosen to keep the model within the 2 million parameter limit while still allowing it to learn effectively from the dataset. Furthermore I tried different configurations of the model, such as varying the number of layers and attention heads, and found out that this configuration provided a good balance between complexity and performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 90,447\n",
      "Vocabulary size: 15\n",
      "Max sequence length: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 18:02:33.006222: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/enrico/Desktop/Infix-to-postfix-notation/.venv/lib/python3.12/site-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'positional_encoding' (of type PositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/enrico/Desktop/Infix-to-postfix-notation/.venv/lib/python3.12/site-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'positional_encoding_1' (of type PositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Encoding Layer for adding positional information to input embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create positional encoding matrix\n",
    "        pe = np.zeros((self.max_len, self.d_model))\n",
    "        position = np.arange(0, self.max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(\n",
    "            np.arange(0, self.d_model, 2) * -(np.log(10000.0) / self.d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        self.pe = self.add_weight(\n",
    "            name=\"positional_encoding\",\n",
    "            shape=(self.max_len, self.d_model),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=False,\n",
    "        )\n",
    "        self.pe.assign(pe)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        return x + self.pe[:seq_len, :]\n",
    "\n",
    "\n",
    "def create_transformer_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    embedding_dim=64,\n",
    "    num_heads=4,\n",
    "    feedforward_dimension=128,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create the Transformer model architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    # INPUTS\n",
    "    encoder_inputs = layers.Input(shape=(max_len,), name=\"encoder_inputs\")\n",
    "    decoder_inputs = layers.Input(shape=(max_len,), name=\"decoder_inputs\")\n",
    "\n",
    "    # EMBEDDING + POSITIONAL ENCODING\n",
    "    encoder_embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(\n",
    "        encoder_inputs\n",
    "    )\n",
    "    encoder_pos_encoding = PositionalEncoding(max_len, embedding_dim)(encoder_embedding)\n",
    "    encoder_output = layers.Dropout(0.1)(encoder_pos_encoding)\n",
    "\n",
    "    decoder_embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(\n",
    "        decoder_inputs\n",
    "    )\n",
    "    decoder_pos_encoding = PositionalEncoding(max_len, embedding_dim)(decoder_embedding)\n",
    "    decoder_output = layers.Dropout(0.1)(decoder_pos_encoding)\n",
    "\n",
    "    # ENCODER STACK\n",
    "    for _ in range(num_encoder_layers):\n",
    "        # Multi-head self-attention\n",
    "        attn_out = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embedding_dim // num_heads, dropout=0.1\n",
    "        )(encoder_output, encoder_output)\n",
    "        attn_out = layers.Dropout(0.1)(attn_out)\n",
    "        encoder_output = layers.LayerNormalization()(encoder_output + attn_out)\n",
    "\n",
    "        # Feed-forward\n",
    "        ffn = layers.Dense(feedforward_dimension, activation=\"relu\")(encoder_output)\n",
    "        ffn = layers.Dense(embedding_dim)(ffn)\n",
    "        ffn = layers.Dropout(0.1)(ffn)\n",
    "        encoder_output = layers.LayerNormalization()(encoder_output + ffn)\n",
    "\n",
    "    # DECODER STACK\n",
    "    for _ in range(num_decoder_layers):\n",
    "        # Masked self-attention (causal)\n",
    "        self_attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embedding_dim // num_heads, dropout=0.1\n",
    "        )(decoder_output, decoder_output, use_causal_mask=True)\n",
    "        self_attn = layers.Dropout(0.1)(self_attn)\n",
    "        decoder_output = layers.LayerNormalization()(decoder_output + self_attn)\n",
    "\n",
    "        # Cross-attention encoder->decoder\n",
    "        cross_attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embedding_dim // num_heads, dropout=0.1\n",
    "        )(decoder_output, encoder_output)\n",
    "        cross_attn = layers.Dropout(0.1)(cross_attn)\n",
    "        decoder_output = layers.LayerNormalization()(decoder_output + cross_attn)\n",
    "\n",
    "        # Feed-forward\n",
    "        ffn_dec = layers.Dense(feedforward_dimension, activation=\"relu\")(decoder_output)\n",
    "        ffn_dec = layers.Dense(embedding_dim)(ffn_dec)\n",
    "        ffn_dec = layers.Dropout(0.1)(ffn_dec)\n",
    "        decoder_output = layers.LayerNormalization()(decoder_output + ffn_dec)\n",
    "\n",
    "    # OUTPUT LAYER\n",
    "    outputs = layers.Dense(vocab_size, activation=\"softmax\")(decoder_output)\n",
    "\n",
    "    model = models.Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = create_transformer_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    embedding_dim=64,  # embedding dimension\n",
    "    num_heads=4,  # Number of attention heads\n",
    "    feedforward_dimension=128,  # Feed-forward dimension\n",
    "    num_encoder_layers=1,  # Number of encoder layers\n",
    "    num_decoder_layers=1,  # Number of decoder layers\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Max sequence length: {MAX_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 10000\n",
      "Validation samples: 10000\n",
      "Input shape: (10000, 30)\n",
      "Output shape: (10000, 30)\n",
      "Decoder input shape: (10000, 30)\n",
      "Loaded pre-trained weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico/Desktop/Infix-to-postfix-notation/.venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 94 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "batch_size = 32\n",
    "epochs = 50  # 50 epochs because we use early stopping\n",
    "MAX_DATASET_SIZE = 10000  # Original size for full training\n",
    "\n",
    "# Generate smaller dataset for initial testing\n",
    "X_train, Y_train = generate_dataset(MAX_DATASET_SIZE)\n",
    "decoder_input_train = shift_right(Y_train)\n",
    "\n",
    "# Validation dataset\n",
    "X_val, Y_val = generate_dataset(MAX_DATASET_SIZE)\n",
    "decoder_input_val = shift_right(Y_val)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "print(f\"Output shape: {Y_train.shape}\")\n",
    "print(f\"Decoder input shape: {decoder_input_train.shape}\")\n",
    "\n",
    "# Display model summary\n",
    "# model.summary()\n",
    "\n",
    "# Early stopping callback to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "LOAD_WEIGHTS = True  # If False, proceed to train the model\n",
    "\n",
    "\n",
    "if LOAD_WEIGHTS:\n",
    "    # Use gdown to download pre-trained weights\n",
    "    import gdown\n",
    "    import os\n",
    "\n",
    "    WEIGHTS_URL = (\n",
    "        \"https://drive.google.com/uc?id=1XDBmXJ6WarbWKhmCDKrUEVB2yqDuQ-EZ\"\n",
    "    )\n",
    "    WEIGHTS_PATH = \"transformer_model.weights.h5\"\n",
    "    if not os.path.exists(WEIGHTS_PATH):\n",
    "        print(\"Pre-trained weights not found\")\n",
    "        gdown.download(WEIGHTS_URL, WEIGHTS_PATH, quiet=False)\n",
    "    model.load_weights(WEIGHTS_PATH)\n",
    "    print(\"Loaded pre-trained weights.\")\n",
    "else:\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X_train, decoder_input_train],\n",
    "        Y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=([X_val, decoder_input_val], Y_val),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_WEIGHTS = False\n",
    "# Save weights\n",
    "if SAVE_WEIGHTS:\n",
    "    model.save_weights(WEIGHTS_PATH)\n",
    "    print(f\"Model weights saved to '{WEIGHTS_PATH}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_decode(model, encoder_input, max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Autoregressively decode a sequence using the trained model.\n",
    "    \"\"\"\n",
    "    encoder_input = np.expand_dims(encoder_input, axis=0)  # shape (1, seq_len)\n",
    "    decoder_input = np.array([[SOS_ID]])  # start with SOS token\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        predictions = model.predict([encoder_input, decoder_input], verbose=0)\n",
    "        next_token_id = np.argmax(predictions[0, -1, :])  # get last token prediction\n",
    "        if next_token_id == EOS_ID:  # stop if EOS is generated\n",
    "            break\n",
    "        decoder_input = np.append(decoder_input, [[next_token_id]], axis=1)\n",
    "\n",
    "    return decoder_input[0]  # return as flat array of token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico/Desktop/Infix-to-postfix-notation/.venv/lib/python3.12/site-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round= 1\n",
      "round= 2\n",
      "round= 3\n",
      "round= 4\n",
      "round= 5\n",
      "round= 6\n",
      "round= 7\n",
      "round= 8\n",
      "round= 9\n",
      "score= 1.0 std= 0.0\n"
     ]
    }
   ],
   "source": [
    "def test(no=20,rounds=10):\n",
    "  rscores =[]\n",
    "  for i in range(rounds):\n",
    "    print(\"round=\",i)\n",
    "    X_test, Y_test = generate_dataset(no)\n",
    "    scores = []\n",
    "    for j in range(no):\n",
    "      encoder_input=X_test[j]\n",
    "      generated = autoregressive_decode(model, encoder_input)[1:] #remove SOS\n",
    "      scores.append(prefix_accuracy_single(Y_test[j], generated, id_to_token))\n",
    "    rscores.append(np.mean(scores))\n",
    "  return np.mean(rscores),np.std(rscores)\n",
    "\n",
    "res, std = test(20,10)\n",
    "print(\"score=\",res,\"std=\",std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxxXPqKQ86fZ"
   },
   "source": [
    "Be sure to evalutate the generator: your model may only take as input the expression in infix format and return its translation to postifix.\n",
    "\n",
    "If you are usuing an encoder-decoder model, generation must be done autoregressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOBottQI9o1h"
   },
   "source": [
    "# What to deliver\n",
    "\n",
    "As usual you are supposed to deliver a single notebook witten in Keras. You are auhtorized to use Keras3 with pytorch as backend if your prefer.\n",
    "\n",
    "Do no upload a zip file: the submission will be rejected.\n",
    "\n",
    "The python notebook should have a clear documentation of the training phase, possibly with its history.\n",
    "\n",
    "You should be able to provide the network paramters upon request. Even better, consider a way to upload them inside your notebook using gdown."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
